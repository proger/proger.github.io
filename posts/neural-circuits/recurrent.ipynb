{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Neural Circuits\n",
    "author: \"Volodymyr Kyrylov\"\n",
    "date: \"2023-10-09\"\n",
    "jupyter: python3\n",
    "categories: [algebra]\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a collection of equations for neural circuits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward\n",
    "\n",
    "$ h_t = F(x_t) $\n",
    "\n",
    "A feed forward with one hidden layer and a nonlinearity is already a [universal approximator](https://wilab.org.ua/approx.pdf), when $F(x)_i = \\sigma(\\langle w_i, x \\rangle) = \\sigma(\\sum_j w_{ij} x_j)$. Here $w_i$ is an i-th row vector of some parameter matrix $W$, and $\\sigma$ is some nonlinearity function.\n",
    "\n",
    "Searching for approximations of this form for most functions requires a lot of time. The efficiency of the search depends on the choice of $\\sigma$, the size and initialization (choice of family) of $W$. In practice we would like to add an inductive bias to the computation to reflect the structure of the data.\n",
    "\n",
    "Also, dropping nonlinearities $\\sigma$ is useful for algebra purposes, as linear operations tend to distribute over each other, which allows for efficient design of computations. Nonlinearity can be recovered through other means like [autoregression](https://arxiv.org/abs/2309.06979). I'm going to mostly ignore nonlinearities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making future timesteps dependent on the past gives a recurrent:\n",
    "\n",
    "### Recurrent\n",
    "\n",
    "$h_t = F(x_t) + H(h_{t-1})$\n",
    "\n",
    "The nonlinearity is usually applied after $+$. A recurrent network can be seen as a shared weights deep feed forward network where future inputs are added during later layers.\n",
    "\n",
    "Deep networks suffer from [gradient vanishing or explosion during training](https://ml.jku.at/publications/older/ch7.pdf). A way to mitigate gradient issues is to replace applications of the chain rule of gradients with the sum rule, like is done in the [highway](https://arxiv.org/abs/1505.00387) below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highway\n",
    "\n",
    "$h_t = C(x_t) \\cdot x_t + T(x_t) \\cdot F(x_t)$\n",
    "\n",
    "where $C$ reads _carry gate_ and $T$ reads _transform gate_. Circuits in gate position usually use the $\\sigma$ nonlinearity with range 0-1. These gates explicitly control what information is taken from what \"branch\" of the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a highway with identity gates gives the [residual](https://arxiv.org/abs/1505.00387):\n",
    "\n",
    "### Residual\n",
    "\n",
    "$h_t = x_t + F(x_t)$\n",
    "\n",
    "The residual highlights the importance of $+$, as gradient $\\nabla$ is a linear operator that distributes over $+$. $h$ can be thought of a [residual stream](https://transformer-circuits.pub/2021/framework/index.html), with which each layer $t$ communicates by reading and writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining a highway with recurrence gives an [LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf). Notably, highways were likely derived by removing recurrence from LSTM however thinking in reverse makes it easier for me personally.\n",
    "\n",
    "### Long Short Term Memory\n",
    "\n",
    "$c_t = C(x_t,h_{t-1}) \\cdot c_{t-1} + T(x_t,h_{t_1}) \\cdot F(x_t,h_{t-1})$\n",
    "\n",
    "$h_t = O(x_t,h_{t-1}) \\cdot c_{t}$\n",
    "\n",
    "where $C$ is also named _forget gate_, $T$ is _input gate_, and $O$ is _output gate_. In this context the *gates are recurrent*.\n",
    "\n",
    "Using feed forward gates without recurrence makes a [quasi LSTM](https://arxiv.org/abs/2309.11197). Quasi LSTM is designed so that equations for future hidden states can be precomputed up front for parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Illustration of Gradient Highway](highway.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
