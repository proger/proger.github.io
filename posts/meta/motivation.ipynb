{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Sparsely supervised learning needs faster learning algorithms\n",
    "author: \"Volodymyr Kyrylov\"\n",
    "date: \"2023-10-07\"\n",
    "jupyter: python3\n",
    "categories: [dreams]\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence recognition and translation tasks from source language $p(x)$ to target language $p(y)$ require maintenance of large consistent parallel corpora $p(y|x)$ to make maximum likelihood estimation methods work.\n",
    "\n",
    "Drifts in target languages (e.g. recently acquired consensus on new spelling or grammar rules, [new words](https://twitter.com/NYT_first_said)) or in corpora curation rules (e.g. decision to include punctuation in speech recognition output) require actions like dataset balancing, retraining or costly hyperparameter retuning to account for substantial changes in the data distribution.\n",
    "\n",
    "Drifts in source languages, such as shifts in speaker characteristic like accents or code switching, channel characteristics like environment noise or reverbertaion, or capture device conditions like EMG body sensor positions require extensive multi-condition training efforts (augmentation design) and condition adaptation.\n",
    "\n",
    "To make deep learning work well in all source and target language conditions users need to ensure that all data is properly curated and balanced, on top of having to deal with architecture-specific issues like sequence length generalization or hyperparameter tuning due to dataset size mismatch.\n",
    "\n",
    "The curator is required to decide what parts of data distribution require attention to improve performance. The objective of getting the system to perform better on a held out test set can be with odds to the objective of equally representing all users.\n",
    "\n",
    "Curation of consistent parallel corpora is a tedious task that requires that a large batch of updates (e.g. 1-10 hours worth of sentences) is done offline, and only then a model is retrained or fine-tuned using a slow learning algorithm like SGD. Getting an SGD-based system to immediately adjust its output based on a single training example (e.g. correction of a single) word is very hard. Even though [SGD converges exponentially fast](https://fa.bianp.net/blog/2021/exponential-sgd/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current deep learning systems work quite well on estimating densities like $p(x)$ and $p(y)$ and with addition of discrete tokenizers it becomes much easier to estimate $p(x,y)$ jointly.\n",
    "\n",
    "It's often easy small amounts of $x$ and $y$ and perform density adaptation of large pretrained models with adapters.\n",
    "\n",
    "Adaptation of translation rules seems to be much harder to get right and requires a lot of tweaking. Given densities, we would like to perform translation, synthesis or recognition. Can we delay binding of translation rules so that users can bring their own?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
