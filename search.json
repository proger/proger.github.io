[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/counter/counter.html",
    "href": "posts/counter/counter.html",
    "title": "How To Count Experience",
    "section": "",
    "text": "When performing exploration of environments we’d like to know how many times we’ve been in the current state.\nimport math\nimport matplotlib.pyplot as plt\nimport string\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ntorch.manual_seed(0)\ntorch.set_grad_enabled(False);"
  },
  {
    "objectID": "posts/counter/counter.html#idea-0-count-numbers",
    "href": "posts/counter/counter.html#idea-0-count-numbers",
    "title": "How To Count Experience",
    "section": "Idea 0: count numbers",
    "text": "Idea 0: count numbers\nAssume all experience is finite and discrete.\nIn this example, given a buffer x of \\(T=7\\) experience of kinds 0..10 exclusive, use torch.unique to count occurrences.\n\nT = 7\nexperience = torch.randint(0, 10, (T,))\nexperience\n\nunique_experience, counts = torch.unique(experience, return_counts=True)\nhisto = torch.zeros(10, dtype=torch.long).scatter_(0, unique_experience, counts)\nplt.subplots(1, 1, figsize=(5,3))\nplt.bar(torch.arange(10), histo, width=0.9, color='#18b6f4')\nplt.xticks(range(10), 'αβγδεζηθικ')\nplt.xlabel('experiences')\nplt.yticks(range(0, max(counts)+1))\nplt.ylabel('occurrences')\nplt.box(False);\n\n\n\n\nWhen our experience happens to be infinite and multidimensional, let’s force it to be finite, discrete and one-dimensional."
  },
  {
    "objectID": "posts/counter/counter.html#idea-1-quantize-vectors",
    "href": "posts/counter/counter.html#idea-1-quantize-vectors",
    "title": "How To Count Experience",
    "section": "Idea 1: quantize vectors",
    "text": "Idea 1: quantize vectors\nDefine a fixed codebook with \\(V\\) experience prototypes. Given a query experience match the closest prototype using inner product or p-norm, then count prototype identifiers.\nBelow I’m going to show an example of a randomly initialized codebook.\n\nV, D = 16, 32\ntable = nn.Embedding(V, D) \nwords = table.weight.detach().numpy()\nplt.figure(figsize=(12,3))\nplt.imshow(words, interpolation='nearest')\nplt.yticks(range(len(words)), string.printable[:len(words)])\nplt.xticks([2**i for i in range(5)])\nplt.title('Codebook of 16 words, 32 dimensions per word vector')\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x7fb33d216080&gt;\n\n\n\n\n\nNow given a random experience \\(x\\) I’m going to use a fast algorithm to compute pairwise L2 norms with the vocabulary. It’s mentioned in Kim, Papamakarios and Mnih, 2020 and this CSCS tutorial on HPC for Python.\n\nx = torch.randn(T, D) # experience\ny = table.weight.data # discrete prototypes\n\nl2_norm_similarity_slow = torch.norm(x[:, None, :] - y[None, :, :], p=2, dim=-1) # (T, V)\nl2_norm_similarity_fast = ((x*x).sum(dim=-1)[:,None] - 2*(x @ y.T) + (y*y).sum(dim=-1)[None, :]).abs().sqrt() # (T,V)\ntorch.allclose(l2_norm_similarity_slow, l2_norm_similarity_fast)\n\nTrue\n\n\nNow argmin gives us our hard assignment to codebook ids:\n\nl2_norm_similarity_fast.argmin(dim=-1)\n\ntensor([ 4, 10,  6,  7, 15,  8,  8])\n\n\n“Good” prototype vectors can be estimated from data using algorithms like LBG, Lloyd’s (aka k-means), CBOW in Mikolov et al, 2013, next word prediction, etc.\nHowever, sometimes random codebooks are enough.\nIn reinforcement learning, random projections of observations are used in a variant of artificial curiosity. An agent is rewarded proportional to the prediction error of a random projection of the state in Burda et al, 2018.\nConsistency objective of random targets is used to bootstrap self-supervised representations in Tarvainen & Valpola, 2017. Random projection quantization is used for large scale self-supervised learning in speech recognition in Chiu et al, 2022 to make learning targets.\nHard assignment, however, is not differentiable. To backpropagate through hard assignment you need to use algorithms like REINFORCE. Such estimation gives very noisy gradient estimates: in REINFORCE you only mark assignments that “worked” and ignore negative information."
  },
  {
    "objectID": "posts/counter/counter.html#idea-2-count-softly",
    "href": "posts/counter/counter.html#idea-2-count-softly",
    "title": "How To Count Experience",
    "section": "Idea 2: count softly",
    "text": "Idea 2: count softly\nMatching can be done against noisy versions of vector observations. Example below uses dot product attention to match and count.\nAlternatively 2-norm can be used instead of the dot product. L2 attention has bounded gradient, as shown in Kim, Papamakarios and Mnih, 2020. This fact can be useful later.\n\nkeys = table(experience) # keys is the experience sequence of length T\nquery = table(torch.arange(len(words))) # keys are the whole vocabulary V\natt = (keys @ query.T) / math.sqrt(D) # (T, V)\natt = att.softmax(dim=-1)\nsoft_counts = att.sum(dim=0)\n\nfig, (axl,axr) = plt.subplots(1, 2, figsize=(12, 3))\naxl.imshow(att)\n\naxl.set_yticks(ticks=range(len(experience)), labels=experience.tolist());\naxl.set_xticks(ticks=range(len(query)), labels=string.printable[:len(query)]);\naxl.set_ylabel('experience is key')\naxl.set_xlabel('vocabulary is query')\naxr.bar(range(len(query)), soft_counts.round().tolist(), alpha=0.3)\naxr.bar(range(len(query)), soft_counts.tolist(), alpha=0.3)\naxr.set_xticks(ticks=range(len(query)), labels=string.printable[:len(query)]);\naxr.set_yticks(ticks=range(3));\naxr.set_ylabel('(rounded) soft counts')\naxr.set_xlabel('vocabulary');\n\n\n\n\nAttention requires us to store all experience in memory, and to compute the attention weights for all experience in memory. This is a quadratic cost in the number of experience. This is not scalable. We need to find a way to approximate the attention weights."
  },
  {
    "objectID": "posts/counter/counter.html#idea-3-running-counts",
    "href": "posts/counter/counter.html#idea-3-running-counts",
    "title": "How To Count Experience",
    "section": "Idea 3: running counts",
    "text": "Idea 3: running counts\nWe perform counting recurrently, updating a hidden state of word counts. This downgrades complexity of updates from quadratic to linear.\nOnce observations are discrete (e.g. turned discrete through elaborate quantization), this can be implemented with cumsum.\n\nB, T, V = 2, 7, 4\nobservations = torch.randint(0, V, (B, T)).view(-1)\none_hot_observations = F.one_hot(observations).view(B,T,V)\nrunning_counts = one_hot_observations.cumsum(dim=-2)\nrunning_counts\n\ntensor([[[0, 1, 0, 0],\n         [0, 2, 0, 0],\n         [0, 3, 0, 0],\n         [1, 3, 0, 0],\n         [1, 3, 1, 0],\n         [1, 3, 2, 0],\n         [2, 3, 2, 0]],\n\n        [[0, 0, 1, 0],\n         [0, 0, 1, 1],\n         [0, 0, 1, 2],\n         [1, 0, 1, 2],\n         [1, 0, 1, 3],\n         [2, 0, 1, 3],\n         [2, 1, 1, 3]]])"
  },
  {
    "objectID": "posts/counter/counter.html#idea-4-abstract-over-embedding-tables",
    "href": "posts/counter/counter.html#idea-4-abstract-over-embedding-tables",
    "title": "How To Count Experience",
    "section": "Idea 4: abstract over embedding tables",
    "text": "Idea 4: abstract over embedding tables\nDifferent environments provide different possible “vocabularies” of experience. All vocabularies are sampled from \\(\\mathcal{N}(0,I)\\). Let’s train a recurrent network to predict how many times it has seen a given input plus one.\n\n# spot init.normal_\nnn.Embedding.reset_parameters??\n\nSignature: nn.Embedding.reset_parameters(self) -&gt; None\nDocstring: &lt;no docstring&gt;\nSource:   \n    def reset_parameters(self) -&gt; None:\n        init.normal_(self.weight)\n        self._fill_padding_idx_with_zero()\nFile:      ~/curiosity/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py\nType:      function\n\n\n\ndef make_batch():\n    \"\"\"\n    Generate random observation sequences and a class index\n    that tells if we've seen the observation once (class 0), twice (class 1) or many times (class 2) before.\n\n    Returns:\n    inputs (torch.Tensor): A tensor of shape (B, T, D) containing the embeddings\n    of the observation sequences, where B is the batch size, T is the sequence length,\n    and D is the embedding dimension.\n\n    targets (torch.Tensor): A tensor of shape (B, T) containing the class index,\n    where B is the batch size and T is the sequence length.\n    \"\"\"\n    B, T, D, Vlocal = 32, 7, 32, 4\n    Vglobal = B*Vlocal\n    local_word_ids = torch.randint(0, 4, (B, T,))\n    table = nn.Embedding(Vglobal, D)\n    global_word_ids = torch.arange(B)[:, None]*4 + local_word_ids\n    inputs = table(global_word_ids)\n    current_input = F.one_hot(local_word_ids.view(-1), num_classes=Vlocal).view(B, T, Vlocal)\n    counts = current_input.cumsum(dim=1)\n\n    # how many times have we seen the current input so far? one, two or many\n    targets = (counts * current_input).clamp(0, 3)\n    targets = targets.sum(-1) - 1\n    return inputs, targets\n\nI’ll be using LSTM that has an inductive bias towards counting thanks to its forget gates. This has been mentioned in Deletang et al, 2023 putting LSTM in context with other architectures.\n\nclass Counter(nn.Module):\n    def __init__(self, dim=32, hidden=512):\n        super().__init__()\n        self.readin = nn.Linear(dim, hidden)\n        self.rnn = nn.LSTM(hidden, hidden, batch_first=True)\n        self.readout = nn.Linear(hidden, 3) # one, two or many times\n\n    def forward(self, x):\n        x = self.readin(x)\n        x, _ = self.rnn(x)\n        x = self.readout(x)\n        return x\n\ntorch.set_grad_enabled(True)\ndevice = 'cuda:1'\ncounter = Counter().to(device)\noptimizer = torch.optim.Adam(counter.parameters(), lr=1e-3)\n\nN = 10000\nlosses = torch.zeros(N)\n\nfor step in range(1,N+1):\n    inputs, targets = make_batch()\n    inputs, targets = inputs.to(device), targets.to(device)\n    logits = counter(inputs)\n    loss = F.cross_entropy(logits.view(-1, 3), targets.view(-1))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    losses[step-1] = loss.item()\n    if step &gt; 1 and math.log10(step) % 1 == 0:\n        plt.figure(figsize=(6,2))\n        plt.plot(losses[:step-1].numpy())\n        plt.title(f'step {step}: loss {loss:.5f}')\n        plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let’s test the counter.\n\ninputs, targets = make_batch()\ncounter(inputs[0, :].to(device)).argmax(dim=-1), targets[0, :].to(device)\n\n(tensor([0, 1, 0, 2, 1, 2, 0], device='cuda:1'),\n tensor([0, 1, 0, 2, 1, 2, 0], device='cuda:1'))\n\n\nThis almost gives us a generic counter of experience taken from a standard 32-dimensional normal distribution.\nHow to predict actual counts? One way is to perform regression to numeric targets.\nIn classification land we can take a softmax, however there is no inductive bias in the softmax towards relationships between output classes, which is why I took classes one, two and many to demonstrate the concept to begin with.\nA way to add such bias is to use windowed label smoothing around the target (a trick from somewhere on kaggle, I’ll need to find a link to it). Another way is to use mixture of logistics from Salimans et al, 2017."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Hello World",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ε␂",
    "section": "",
    "text": "How To Count Experience\n\n\n\n\n\n\n\nrl\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nVolodymyr Kyrylov\n\n\n\n\n\n\n  \n\n\n\n\nHello World\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\nVolodymyr Kyrylov\n\n\n\n\n\n\nNo matching items"
  }
]